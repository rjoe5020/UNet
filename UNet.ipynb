{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image segmentation (with U-Nets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from os import listdir\n",
    "import skimage\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Architecture\n",
    "![architecture](.\\architecture.png \"UNet architecture\")\n",
    "\n",
    "### Contracting Path\n",
    "It consits of 4 blocks and every block has the same architecture (typical architecture of a concolutional network)\n",
    "1. Two 3x3 unpadded convolutions each followed by a rectified linear unit\n",
    "    - because its is unpadded we loss with every convolution 2 pixels in each dimension\n",
    "2. 2x2 max pooling operations with stride 2\n",
    "    - with this setting the image get downsampled by a factor of 2\n",
    "\n",
    "They start with 64 feature channels (filters) at the first block and with each block they double the number of feature channels\n",
    "\n",
    "The output of the last block will be sended through two convoltuion layers (3x3) each followed by a ReLU, because of the doubling of the feature channels the resulting feature map for each layer is 1024 dimensional.\n",
    "\n",
    "### Expansive Path\n",
    "Its symetric to the contracting path, so again 4 blocks. However the architecture changes slightly\n",
    "1. Upsampling by a 2x2 up-convolution (that halves the number of feature channels)\n",
    "2. Copy a cropped version of the feature map from the corresponding feature map of the contracting path and concatenate with the upsampled feature map\n",
    "    - The cropping is necessary due to the loss of border pixels in every convolution\n",
    "3. Two 3x3 unpadded convolutions each followed by a rectified linear unit\n",
    "\n",
    "At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes (in our case 2 classes). \n",
    "\n",
    "In total the network has 23 convolutional layers. (2\\*4 (Contracting Path) + 2 (last layer) + 3\\*4 (Expansive Path) + 1 (final layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because 18 of the 23 convolutional layers uses the same setting, we defined a own function for this\n",
    "def conv3x3(_input, output):\n",
    "    return nn.Conv2d(_input, output, 3, padding=1)\n",
    "\n",
    "def conv_upsample(in_channels,channels,i):\n",
    "    # a = (i+2p-k)mod s\n",
    "    s = 2\n",
    "    k = 2\n",
    "    p = 0\n",
    "    a = np.mod(i+2*p-k,s)\n",
    "    return nn.ConvTranspose2d(in_channels=in_channels, out_channels=channels, kernel_size=k, stride=s, output_padding=a)\n",
    "\n",
    "\n",
    "# define block of contracting path\n",
    "class ContractingBlock(nn.Module):\n",
    "    def __init__(self,in_channels,channels):\n",
    "        super(ContractingBlock, self).__init__()\n",
    "        # for the first conv layer the number of input channels are the number of channels form the previous block and they will be doubled (first block starts with 64) \n",
    "        self.conv1 = conv3x3(in_channels,channels)\n",
    "        self.conv2 = conv3x3(channels,channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample_block = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x, isInitBlock = 0):\n",
    "        if not isInitBlock:\n",
    "            x = self.downsample_block(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        return x\n",
    "\n",
    "# define block of expansive path\n",
    "class ExpansiveBlock(nn.Module):\n",
    "    def __init__(self, in_channels, channels,target_size):\n",
    "        super(ExpansiveBlock, self).__init__()\n",
    "        self.upsampled = conv_upsample(in_channels, channels,target_size)\n",
    "        self.conv1 = conv3x3(in_channels,channels)\n",
    "        self.conv2 = conv3x3(channels,channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x, cop):\n",
    "        x = self.upsampled(x)\n",
    "        #n, c, h, w = x.size()\n",
    "        #cop = cop[0:n,0:c,0:h,0:w]\n",
    "        x = torch.cat([cop, x], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, nrChannels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.contracted1 = ContractingBlock(3,nrChannels)\n",
    "        self.contracted2 = ContractingBlock(nrChannels,2*nrChannels)\n",
    "        self.contracted3 = ContractingBlock(2*nrChannels,4*nrChannels)\n",
    "        self.contracted4 = ContractingBlock(4*nrChannels,8*nrChannels)\n",
    "        self.bottom = ContractingBlock(8*nrChannels,16*nrChannels)\n",
    "        self.expanded1 = ExpansiveBlock(16*nrChannels,8*nrChannels,73)\n",
    "        self.expanded2 = ExpansiveBlock(8*nrChannels,4*nrChannels,146)\n",
    "        self.expanded3 = ExpansiveBlock(4*nrChannels,2*nrChannels,292)\n",
    "        self.expanded4 = ExpansiveBlock(2*nrChannels,nrChannels,584)\n",
    "        self.final = nn.Conv2d(nrChannels, 2, 1, padding=0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        con1 = self.contracted1(x, 1)\n",
    "        con2 = self.contracted2(con1)\n",
    "        con3 = self.contracted3(con2)\n",
    "        con4 = self.contracted4(con3)\n",
    "        bot = self.bottom(con4)\n",
    "        exp1 = self.expanded1(bot,con4)\n",
    "        exp2 = self.expanded2(exp1,con3)\n",
    "        exp3 = self.expanded3(exp2,con2)\n",
    "        exp4 = self.expanded4(exp3,con1)\n",
    "        fin = self.final(exp4)\n",
    "        \n",
    "        return fin\n",
    "\n",
    "unet = UNet(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss-Function\n",
    "For the loss a pixel-wise soft-max over the final feature map combined with the corss entropy loss function is computed.\n",
    "The function torch.nn.functional.corss_entropy() does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EngergyFunction(featuremap, target):\n",
    "    featuremap = featuremap.transpose(1, 2).transpose(2, 3).contiguous().view(-1, 2)\n",
    "    target = target/255\n",
    "    target = target+1\n",
    "    target = target.contiguous().view(-1)\n",
    "    loss = F.cross_entropy(featuremap, target,reduction='elementwise_mean',)\n",
    "    \n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Like described in [1] we use stochastic gradient descent with a momentum of 0.99 for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "        unet.parameters(), \n",
    "        lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLister(image_path,label_path):\n",
    "    data = listdir(image_path)\n",
    "    labels = listdir(label_path)    \n",
    "    combined = list()\n",
    "    for d, l in zip(data, labels):\n",
    "        combined.append((image_path + d, label_path + l))\n",
    "        \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(data.Dataset):\n",
    "    def __init__(self, datalist):\n",
    "        self.datalist = datalist\n",
    "        self.transforms = transforms.ToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.datalist[index][0])\n",
    "        image = self.transforms(image)\n",
    "        # padding for quadratic input\n",
    "        image = F.pad(image, (19,0), 'constant', 0)\n",
    "        label = Image.open(self.datalist[index][1])\n",
    "        label = self.transforms(label)\n",
    "        label = label.type(torch.LongTensor)\n",
    "        label = F.pad(label, (19,0), 'constant', 0)\n",
    "        return image,label\n",
    "\n",
    "train_image_path = './data/training_copy/images/'\n",
    "train_label_path = './data/training_copy/mask/'\n",
    "test_image_path = './data/test/images/'\n",
    "test_label_path = './data/test/1st_manual/'\n",
    "trainlist = dataLister(train_image_path,train_label_path)\n",
    "train_dataset = SegmentationDataset(trainlist)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "testlist = dataLister(test_image_path,test_label_path)\n",
    "test_dataset = SegmentationDataset(testlist)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philm\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:16: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "unet.train()\n",
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = unet(inputs)\n",
    "        loss = EngergyFunction(output,labels)\n",
    "        epoch_loss += loss.item() # logging\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(epoch_loss)\n",
    "    print(epoch,epoch_loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1811e-10, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "test_input, test_label = dataiter.next()\n",
    "test_output = unet(test_input)\n",
    "EngergyFunction(test_output,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EngergyFunction(output,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "test_input, test_label = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.empty([584, 584])\n",
    "for i in range(584):\n",
    "    for j in range(584):\n",
    "        temp1 = np.absolute(output[0,0,i,j].item())\n",
    "        temp2 = np.absolute(output[0,1,i,j].item())\n",
    "        if temp1 > temp2:\n",
    "            img[i,j] = 1;\n",
    "        else:\n",
    "            img[i,j] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADZRJREFUeJzt3X+o3fV9x/Hna4k/uuqMWpWQZIvS/KF/bFaCTbGMzv5AXWn8w4KlYCiBwH6AxUEXNxgU9o/7o4qs2IUpi6OturZikG02RMv2j9Gk/m6quQ7XXBIMRU1XClut7/1xPreeT3Kbe6Ln3HOU5wO+fL/f9/dzz/d9uee+7uf7PV+4qSokacFvTbsBSbPFUJDUMRQkdQwFSR1DQVLHUJDUmUgoJLkmyYtJ5pJsn8Q5JE1Gxv2cQpIVwEvAp4F54EngC1X1o7GeSNJETGKmcCUwV1X/VVX/B9wHbJ7AeSRNwMoJvOYa4NDQ/jzw0ZN9QRIfq5Qm76dVdcFSgyYRClmkdsIvfZJtwLYJnF/S4v57lEGTCIV5YN3Q/lrg8PGDqmoHsAOcKUizZBL3FJ4ENiS5OMnpwI3ArgmcR9IEjH2mUFVvJvlz4BFgBXBPVb0w7vNImoyxfyT5jprw8kFaDvurauNSg3yiUVLHUJDUMRQkdQwFSR1DQVLHUJDUMRQkdQwFSR1DQVLHUJDUMRQkdQwFSR1DQVLHUJDUMRQkdQwFSR1DQVLHUJDUMRQkdQwFSR1DQVLHUJDUMRQkdQwFSR1DQVLHUJDUMRQkdQwFSZ0lQyHJPUmOJnl+qHZekt1JDrb1ua2eJHcmmUvybJIrJtm8pPEbZabwT8A1x9W2A3uqagOwp+0DXAtsaMs24K7xtClpuSwZClX1H8Brx5U3Azvb9k7g+qH6vTXwOLAqyepxNStp8t7pPYWLquoIQFtf2OprgEND4+ZbTdJ7xMoxv14WqdWiA5NtDC4xJM2QdzpTeHXhsqCtj7b6PLBuaNxa4PBiL1BVO6pqY1VtfIc9SJqAdxoKu4AtbXsL8NBQ/ab2KcQm4NjCZYak94iqOukCfBs4AvySwUxgK3A+g08dDrb1eW1sgK8DLwPPARuXev32deXi4jLxZd8ov49pv5RTlWT6TUjvf/tHuVz3iUZJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJnSVDIcm6JI8lOZDkhSQ3t/p5SXYnOdjW57Z6ktyZZC7Js0mumPQ3IWl8RpkpvAn8RVVdCmwC/izJZcB2YE9VbQD2tH2Aa4ENbdkG3DX2riVNzJKhUFVHquqHbft/gAPAGmAzsLMN2wlc37Y3A/fWwOPAqiSrx965pIk4pXsKSdYDHwH2AhdV1REYBAdwYRu2Bjg09GXzrXb8a21Lsi/JvlNvW9KkrBx1YJKzgO8CX66qnyX5jUMXqdUJhaodwI722icclzQdI80UkpzGIBC+WVXfa+VXFy4L2vpoq88D64a+fC1weDztSpq0UT59CHA3cKCqvjZ0aBewpW1vAR4aqt/UPoXYBBxbuMyQNPtSdfKZe5KPA/8JPAe81cp/xeC+wgPA7wI/AT5fVa+1EPl74BrgF8CXquqk9w28fJCWxf6q2rjUoCVDYTkYCtKyGCkUfKJRUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJHUNBUsdQkNQxFCR1DAVJnVH+Ff2ZSZ5I8kySF5J8tdUvTrI3ycEk9yc5vdXPaPtz7fj6yX4LksZplJnC/wJXV9UfAJcD1yTZBNwG3F5VG4DXga1t/Fbg9ar6MHB7GyfpPWLJUKiBn7fd09pSwNXAd1p9J3B9297c9mnHP5kkY+tY0kSNdE8hyYokTwNHgd3Ay8AbVfVmGzIPrGnba4BDAO34MeD8RV5zW5J9Sfa9u29B0jiNFApV9auquhxYC1wJXLrYsLZebFZQJxSqdlTVxqraOGqzkibvlD59qKo3gB8Am4BVSVa2Q2uBw217HlgH0I6fA7w2jmYlTd4onz5ckGRV2/4A8CngAPAYcEMbtgV4qG3vavu0449W1QkzBUmzaeXSQ1gN7EyygkGIPFBVDyf5EXBfkr8FngLubuPvBv45yRyDGcKNE+hb0oRkFv6IJ5l+E9L73/5R7uH5RKOkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6I4dCkhVJnkrycNu/OMneJAeT3J/k9FY/o+3PtePrJ9O6pEk4lZnCzcCBof3bgNuragPwOrC11bcCr1fVh4Hb2zhJ7xEjhUKStcAfA//Y9gNcDXynDdkJXN+2N7d92vFPtvGS3gNGnSncAXwFeKvtnw+8UVVvtv15YE3bXgMcAmjHj7XxnSTbkuxLsu8d9i5pApYMhSSfBY5W1f7h8iJDa4RjbxeqdlTVxqraOFKnkpbFyhHGXAV8Lsl1wJnA7zCYOaxKsrLNBtYCh9v4eWAdMJ9kJXAO8NrYO5c0EUvOFKrq1qpaW1XrgRuBR6vqi8BjwA1t2Bbgoba9q+3Tjj9aVSfMFCTNpnfznMJfArckmWNwz+DuVr8bOL/VbwG2v7sWJS2nzMIf8STTb0J6/9s/yj08n2iU1DEUJHUMBUkdQ0FSx1CQ1DEUJHUMBUkdQ0FSx1CQ1DEUJHUMBUkdQ0FSx1CQ1DEUJHUMBUkdQ0FSx1CQ1DEUJHUMBUkdQ0FSx1CQ1DEUJHUMBUkdQ0FSx1CQ1DEUJHUMBUkdQ0FSZ6RQSPJKkueSPJ1kX6udl2R3koNtfW6rJ8mdSeaSPJvkikl+A5LG61RmCn9UVZcP/dfa7cCeqtoA7OHtfzl/LbChLduAu8bVrKTJezeXD5uBnW17J3D9UP3eGngcWJVk9bs4j6RlNGooFPD9JPuTbGu1i6rqCEBbX9jqa4BDQ18732qS3gNWjjjuqqo6nORCYHeSH59kbBap1QmDBuGybZGxkqZopJlCVR1u66PAg8CVwKsLlwVtfbQNnwfWDX35WuDwIq+5o6o2Dt2jkDQDlgyFJB9McvbCNvAZ4HlgF7ClDdsCPNS2dwE3tU8hNgHHFi4zJM2+US4fLgIeTLIw/ltV9e9JngQeSLIV+Anw+Tb+X4HrgDngF8CXxt61pIlJ1QmX+8vfRDL9JqT3v/2jXK77RKOkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqWMoSOoYCpI6hoKkjqEgqTPqv6KftJ8DL067ieN8CPjptJtYxCz2ZU+jmXZPvzfKoFkJhRdn7V/SJ9k3az3BbPZlT6OZxZ4W4+WDpI6hIKkzK6GwY9oNLGIWe4LZ7MueRjOLPZ0gVTXtHiTNkFmZKUiaEVMPhSTXJHkxyVyS7ct43nuSHE3y/FDtvCS7kxxs63NbPUnubD0+m+SKCfW0LsljSQ4keSHJzdPuK8mZSZ5I8kzr6autfnGSva2n+5Oc3upntP25dnz9uHsa6m1FkqeSPDxDPb2S5LkkTyfZ12pTfV+dsqqa2gKsAF4GLgFOB54BLlumc/8hcAXw/FDt74DtbXs7cFvbvg74NyDAJmDvhHpaDVzRts8GXgIum2Zf7bXPatunAXvbuR4Abmz1bwB/0rb/FPhG274RuH+CP8NbgG8BD7f9WejpFeBDx9Wm+r465e9hqieHjwGPDO3fCty6jOdff1wovAisbturGTw/AfAPwBcWGzfh/h4CPj0rfQG/DfwQ+CiDh3BWHv9zBB4BPta2V7ZxmUAva4E9wNXAw+0Xa6o9tddfLBRm4uc36jLty4c1wKGh/flWm5aLquoIQFtf2OrL3meb4n6EwV/mqfbVpulPA0eB3Qxmd29U1ZuLnPfXPbXjx4Dzx90TcAfwFeCttn/+DPQEUMD3k+xPsq3VZuZ9NYppP9GYRWqz+HHIsvaZ5Czgu8CXq+pnyWKnX76+qupXwOVJVgEPApee5LwT7ynJZ4GjVbU/ySdGOO9y/vyuqqrDSS4Edif58UnGzuT7f9ozhXlg3dD+WuDwlHoBeDXJaoC2Ptrqy9ZnktMYBMI3q+p7s9IXQFW9AfyAwfXvqiQLf1SGz/vrntrxc4DXxtzKVcDnkrwC3MfgEuKOKfcEQFUdbuujDAL0Smbk5zeqaYfCk8CGdtf4dAY3gXZNsZ9dwJa2vYXBNf1C/aZ2t3gTcGxhOjhOGUwJ7gYOVNXXZqGvJBe0GQJJPgB8CjgAPAbc8Bt6Wuj1BuDRahfM41JVt1bV2qpaz+A982hVfXGaPQEk+WCSsxe2gc8AzzPl99Upm/ZNDQZ3YF9icJ3618t43m8DR4BfMkjsrQyuM/cAB9v6vDY2wNdbj88BGyfU08cZTB+fBZ5uy3XT7Av4feCp1tPzwN+0+iXAE8Ac8C/AGa1+Ztufa8cvmfDP8RO8/enDVHtq53+mLS8svJ+n/b461cUnGiV1pn35IGnGGAqSOoaCpI6hIKljKEjqGAqSOoaCpI6hIKnz/51Cs/bVuU/DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ab29f2f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imshow(torchvision.utils.make_grid(test_input))\n",
    "img_01 = Image.fromarray(img,'1')\n",
    "plt.imshow(img_01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.squeeze(test.data.max(1)[0].cpu().numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] U-Net: Convolutional Networks for Biomedical Image Segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
